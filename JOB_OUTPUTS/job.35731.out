Running on desktop1:
INFO:sparkhpc.sparkjob:master command: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-master.sh
INFO:sparkhpc.sparkjob:[[92mstart_cluster] [0mmaster running at spark://172.16.16.101:7077
INFO:sparkhpc.sparkjob:[[92mstart_cluster] [0mmaster UI available at http://desktop1.hpc.itu.dk:8080
I GOT THE SCHEDULER
slurm
master_command: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-master.sh
['desktop1', 'desktop2', 'desktop3']
INFO:sparkhpc.sparkjob:slaves command: srun /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-slave.sh spark://172.16.16.101:7077 -c 1
INFO:sparkhpc.sparkjob:XXX SLAVES SLAVES: srun /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-slave.sh spark://172.16.16.101:7077 -c 1
starting org.apache.spark.deploy.worker.Worker, logging to /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/logs/spark-laal-org.apache.spark.deploy.worker.Worker-1-desktop1.out
starting org.apache.spark.deploy.worker.Worker, logging to /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/logs/spark-laal-org.apache.spark.deploy.worker.Worker-1-desktop1.out
starting org.apache.spark.deploy.worker.Worker, logging to /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/logs/spark-laal-org.apache.spark.deploy.worker.Worker-1-desktop1.out
Spark Command: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java -cp /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://172.16.16.101:7077 -c 1
========================================
Spark Command: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java -cp /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://172.16.16.101:7077 -c 1
========================================
Spark Command: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java -cp /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://172.16.16.101:7077 -c 1
========================================
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/02/25 11:16:35 INFO Worker: Started daemon with process name: 934@desktop1
21/02/25 11:16:35 INFO SignalUtils: Registered signal handler for TERM
21/02/25 11:16:35 INFO SignalUtils: Registered signal handler for HUP
21/02/25 11:16:35 INFO SignalUtils: Registered signal handler for INT
21/02/25 11:16:35 WARN Utils: Your hostname, desktop1 resolves to a loopback address: 127.0.0.1; using 172.16.16.101 instead (on interface eno1)
21/02/25 11:16:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/02/25 11:16:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/25 11:16:36 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:16:36 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:16:36 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:16:36 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:16:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:16:36 INFO Utils: Successfully started service 'sparkWorker' on port 38706.
21/02/25 11:16:36 INFO Worker: Starting Spark worker 172.16.16.101:38706 with 1 cores, 2000.0 MiB RAM
21/02/25 11:16:36 INFO Worker: Running Spark version 3.0.2
21/02/25 11:16:36 INFO Worker: Spark home: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7
21/02/25 11:16:36 INFO ResourceUtils: ==============================================================
21/02/25 11:16:36 INFO ResourceUtils: Resources for spark.worker:

21/02/25 11:16:36 INFO ResourceUtils: ==============================================================
21/02/25 11:16:36 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
21/02/25 11:16:36 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://desktop1.hpc.itu.dk:8081
21/02/25 11:16:36 INFO Worker: Connecting to master 172.16.16.101:7077...
21/02/25 11:16:36 INFO TransportClientFactory: Successfully created connection to /172.16.16.101:7077 after 28 ms (0 ms spent in bootstraps)
21/02/25 11:16:36 INFO Worker: Successfully registered with master spark://172.16.16.101:7077
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/02/25 11:16:37 INFO Worker: Started daemon with process name: 18104@desktop3
21/02/25 11:16:37 INFO SignalUtils: Registered signal handler for TERM
21/02/25 11:16:37 INFO SignalUtils: Registered signal handler for HUP
21/02/25 11:16:37 INFO SignalUtils: Registered signal handler for INT
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/02/25 11:16:37 WARN Utils: Your hostname, desktop3 resolves to a loopback address: 127.0.0.1; using 172.16.16.103 instead (on interface eno1)
21/02/25 11:16:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/02/25 11:16:37 INFO Worker: Started daemon with process name: 13759@desktop2
21/02/25 11:16:37 INFO SignalUtils: Registered signal handler for TERM
21/02/25 11:16:37 INFO SignalUtils: Registered signal handler for HUP
21/02/25 11:16:37 INFO SignalUtils: Registered signal handler for INT
21/02/25 11:16:38 WARN Utils: Your hostname, desktop2 resolves to a loopback address: 127.0.0.1; using 172.16.16.102 instead (on interface eno1)
21/02/25 11:16:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/02/25 11:16:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/25 11:16:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/25 11:16:38 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:16:38 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:16:38 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:16:38 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:16:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:16:38 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:16:38 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:16:38 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:16:38 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:16:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:16:38 INFO Utils: Successfully started service 'sparkWorker' on port 37264.
21/02/25 11:16:38 INFO Utils: Successfully started service 'sparkWorker' on port 35340.
21/02/25 11:16:38 INFO Worker: Starting Spark worker 172.16.16.103:37264 with 1 cores, 2000.0 MiB RAM
21/02/25 11:16:38 INFO Worker: Running Spark version 3.0.2
21/02/25 11:16:38 INFO Worker: Spark home: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7
21/02/25 11:16:38 INFO ResourceUtils: ==============================================================
21/02/25 11:16:38 INFO ResourceUtils: Resources for spark.worker:

21/02/25 11:16:38 INFO ResourceUtils: ==============================================================
21/02/25 11:16:38 INFO Worker: Starting Spark worker 172.16.16.102:35340 with 1 cores, 2000.0 MiB RAM
21/02/25 11:16:38 INFO Worker: Running Spark version 3.0.2
21/02/25 11:16:38 INFO Worker: Spark home: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7
21/02/25 11:16:38 INFO ResourceUtils: ==============================================================
21/02/25 11:16:38 INFO ResourceUtils: Resources for spark.worker:

21/02/25 11:16:38 INFO ResourceUtils: ==============================================================
21/02/25 11:16:38 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
21/02/25 11:16:38 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://desktop3.hpc.itu.dk:8081
21/02/25 11:16:39 INFO Worker: Connecting to master 172.16.16.101:7077...
21/02/25 11:16:39 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
21/02/25 11:16:39 INFO TransportClientFactory: Successfully created connection to /172.16.16.101:7077 after 20 ms (0 ms spent in bootstraps)
21/02/25 11:16:39 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://desktop2.hpc.itu.dk:8081
21/02/25 11:16:39 INFO Worker: Connecting to master 172.16.16.101:7077...
21/02/25 11:16:39 INFO Worker: Successfully registered with master spark://172.16.16.101:7077
21/02/25 11:16:39 INFO TransportClientFactory: Successfully created connection to /172.16.16.101:7077 after 21 ms (0 ms spent in bootstraps)
21/02/25 11:16:39 INFO Worker: Successfully registered with master spark://172.16.16.101:7077
21/02/25 11:42:50 INFO Worker: Asked to launch executor app-20210225114250-0000/1 for pyspark-shell
21/02/25 11:42:50 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:42:50 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:42:50 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:42:50 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:42:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:42:50 INFO Worker: Asked to launch executor app-20210225114250-0000/0 for pyspark-shell
21/02/25 11:42:50 INFO Worker: Asked to launch executor app-20210225114250-0000/2 for pyspark-shell
21/02/25 11:42:50 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:42:50 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:42:50 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:42:50 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:42:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:42:50 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:42:50 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:42:50 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:42:50 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:42:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:42:50 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=45854" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@front-em1.hpc.itu.dk:45854" "--executor-id" "1" "--hostname" "172.16.16.101" "--cores" "1" "--app-id" "app-20210225114250-0000" "--worker-url" "spark://Worker@172.16.16.101:38706"
21/02/25 11:42:50 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=45854" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@front-em1.hpc.itu.dk:45854" "--executor-id" "0" "--hostname" "172.16.16.102" "--cores" "1" "--app-id" "app-20210225114250-0000" "--worker-url" "spark://Worker@172.16.16.102:35340"
21/02/25 11:42:50 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=45854" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@front-em1.hpc.itu.dk:45854" "--executor-id" "2" "--hostname" "172.16.16.103" "--cores" "1" "--app-id" "app-20210225114250-0000" "--worker-url" "spark://Worker@172.16.16.103:37264"
21/02/25 11:43:42 INFO Worker: Asked to kill executor app-20210225114250-0000/1
21/02/25 11:43:42 INFO ExecutorRunner: Runner thread for executor app-20210225114250-0000/1 interrupted
21/02/25 11:43:42 INFO ExecutorRunner: Killing process!
21/02/25 11:43:42 INFO Worker: Asked to kill executor app-20210225114250-0000/2
21/02/25 11:43:42 INFO Worker: Asked to kill executor app-20210225114250-0000/0
21/02/25 11:43:42 INFO ExecutorRunner: Runner thread for executor app-20210225114250-0000/0 interrupted
21/02/25 11:43:42 INFO ExecutorRunner: Killing process!
21/02/25 11:43:42 INFO ExecutorRunner: Runner thread for executor app-20210225114250-0000/2 interrupted
21/02/25 11:43:42 INFO ExecutorRunner: Killing process!
21/02/25 11:43:42 INFO Worker: Executor app-20210225114250-0000/1 finished with state KILLED exitStatus 143
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225114250-0000, execId=1)
21/02/25 11:43:42 INFO Worker: Cleaning up local directories for application app-20210225114250-0000
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Application app-20210225114250-0000 removed, cleanupLocalDirs = true
21/02/25 11:43:42 INFO Worker: Executor app-20210225114250-0000/0 finished with state KILLED exitStatus 143
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225114250-0000, execId=0)
21/02/25 11:43:42 INFO Worker: Cleaning up local directories for application app-20210225114250-0000
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Application app-20210225114250-0000 removed, cleanupLocalDirs = true
21/02/25 11:43:42 INFO Worker: Executor app-20210225114250-0000/2 finished with state KILLED exitStatus 143
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225114250-0000, execId=2)
21/02/25 11:43:42 INFO ExternalShuffleBlockResolver: Application app-20210225114250-0000 removed, cleanupLocalDirs = true
21/02/25 11:43:42 INFO Worker: Cleaning up local directories for application app-20210225114250-0000
21/02/25 11:45:23 INFO Worker: Asked to launch executor app-20210225114523-0001/1 for pyspark-shell
21/02/25 11:45:23 INFO Worker: Asked to launch executor app-20210225114523-0001/0 for pyspark-shell
21/02/25 11:45:23 INFO Worker: Asked to launch executor app-20210225114523-0001/2 for pyspark-shell
21/02/25 11:45:23 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:45:23 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:45:23 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:45:23 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:45:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:45:23 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:45:23 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:45:23 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:45:23 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:45:23 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:45:23 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:45:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:45:23 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:45:23 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:45:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:45:23 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=42256" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:42256" "--executor-id" "1" "--hostname" "172.16.16.101" "--cores" "1" "--app-id" "app-20210225114523-0001" "--worker-url" "spark://Worker@172.16.16.101:38706"
21/02/25 11:45:23 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=42256" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:42256" "--executor-id" "2" "--hostname" "172.16.16.103" "--cores" "1" "--app-id" "app-20210225114523-0001" "--worker-url" "spark://Worker@172.16.16.103:37264"
21/02/25 11:45:23 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=42256" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:42256" "--executor-id" "0" "--hostname" "172.16.16.102" "--cores" "1" "--app-id" "app-20210225114523-0001" "--worker-url" "spark://Worker@172.16.16.102:35340"
21/02/25 11:45:24 INFO Worker: Asked to kill executor app-20210225114523-0001/2
21/02/25 11:45:24 INFO Worker: Asked to kill executor app-20210225114523-0001/0
21/02/25 11:45:24 INFO ExecutorRunner: Runner thread for executor app-20210225114523-0001/0 interrupted
21/02/25 11:45:24 INFO ExecutorRunner: Runner thread for executor app-20210225114523-0001/2 interrupted
21/02/25 11:45:24 INFO ExecutorRunner: Killing process!
21/02/25 11:45:24 INFO ExecutorRunner: Killing process!
21/02/25 11:45:24 INFO Worker: Asked to kill executor app-20210225114523-0001/1
21/02/25 11:45:24 INFO ExecutorRunner: Runner thread for executor app-20210225114523-0001/1 interrupted
21/02/25 11:45:24 INFO ExecutorRunner: Killing process!
21/02/25 11:45:24 INFO Worker: Executor app-20210225114523-0001/1 finished with state KILLED exitStatus 143
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225114523-0001, execId=1)
21/02/25 11:45:24 INFO Worker: Cleaning up local directories for application app-20210225114523-0001
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Application app-20210225114523-0001 removed, cleanupLocalDirs = true
21/02/25 11:45:24 INFO Worker: Executor app-20210225114523-0001/0 finished with state KILLED exitStatus 143
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225114523-0001, execId=0)
21/02/25 11:45:24 INFO Worker: Cleaning up local directories for application app-20210225114523-0001
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Application app-20210225114523-0001 removed, cleanupLocalDirs = true
21/02/25 11:45:24 INFO Worker: Executor app-20210225114523-0001/2 finished with state KILLED exitStatus 143
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225114523-0001, execId=2)
21/02/25 11:45:24 INFO Worker: Cleaning up local directories for application app-20210225114523-0001
21/02/25 11:45:24 INFO ExternalShuffleBlockResolver: Application app-20210225114523-0001 removed, cleanupLocalDirs = true
21/02/25 11:54:28 INFO Worker: Asked to launch executor app-20210225115428-0002/1 for pyspark-shell
21/02/25 11:54:28 INFO Worker: Asked to launch executor app-20210225115428-0002/0 for pyspark-shell
21/02/25 11:54:28 INFO Worker: Asked to launch executor app-20210225115428-0002/2 for pyspark-shell
21/02/25 11:54:28 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:54:28 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:54:28 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:54:28 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:54:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:54:28 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:54:28 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:54:28 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:54:28 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:54:28 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:54:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:54:28 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:54:28 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:54:28 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:54:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:54:28 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=38094" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:38094" "--executor-id" "1" "--hostname" "172.16.16.101" "--cores" "1" "--app-id" "app-20210225115428-0002" "--worker-url" "spark://Worker@172.16.16.101:38706"
21/02/25 11:54:28 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=38094" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:38094" "--executor-id" "2" "--hostname" "172.16.16.103" "--cores" "1" "--app-id" "app-20210225115428-0002" "--worker-url" "spark://Worker@172.16.16.103:37264"
21/02/25 11:54:28 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=38094" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:38094" "--executor-id" "0" "--hostname" "172.16.16.102" "--cores" "1" "--app-id" "app-20210225115428-0002" "--worker-url" "spark://Worker@172.16.16.102:35340"
21/02/25 11:54:28 INFO Worker: Asked to kill executor app-20210225115428-0002/1
21/02/25 11:54:28 INFO Worker: Asked to kill executor app-20210225115428-0002/0
21/02/25 11:54:28 INFO Worker: Asked to kill executor app-20210225115428-0002/2
21/02/25 11:54:28 INFO ExecutorRunner: Runner thread for executor app-20210225115428-0002/1 interrupted
21/02/25 11:54:28 INFO ExecutorRunner: Killing process!
21/02/25 11:54:28 INFO ExecutorRunner: Runner thread for executor app-20210225115428-0002/0 interrupted
21/02/25 11:54:28 INFO ExecutorRunner: Killing process!
21/02/25 11:54:28 INFO ExecutorRunner: Runner thread for executor app-20210225115428-0002/2 interrupted
21/02/25 11:54:28 INFO ExecutorRunner: Killing process!
21/02/25 11:54:28 INFO Worker: Executor app-20210225115428-0002/1 finished with state KILLED exitStatus 143
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115428-0002, execId=1)
21/02/25 11:54:28 INFO Worker: Cleaning up local directories for application app-20210225115428-0002
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Application app-20210225115428-0002 removed, cleanupLocalDirs = true
21/02/25 11:54:28 INFO Worker: Executor app-20210225115428-0002/2 finished with state KILLED exitStatus 143
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115428-0002, execId=2)
21/02/25 11:54:28 INFO Worker: Cleaning up local directories for application app-20210225115428-0002
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Application app-20210225115428-0002 removed, cleanupLocalDirs = true
21/02/25 11:54:28 INFO Worker: Executor app-20210225115428-0002/0 finished with state KILLED exitStatus 143
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115428-0002, execId=0)
21/02/25 11:54:28 INFO ExternalShuffleBlockResolver: Application app-20210225115428-0002 removed, cleanupLocalDirs = true
21/02/25 11:54:28 INFO Worker: Cleaning up local directories for application app-20210225115428-0002
21/02/25 11:55:04 INFO Worker: Asked to launch executor app-20210225115504-0003/1 for pyspark-shell
21/02/25 11:55:04 INFO Worker: Asked to launch executor app-20210225115504-0003/0 for pyspark-shell
21/02/25 11:55:04 INFO Worker: Asked to launch executor app-20210225115504-0003/2 for pyspark-shell
21/02/25 11:55:04 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:55:04 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:55:04 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:55:04 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:55:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:55:04 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:55:04 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:55:04 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:55:04 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:55:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:55:04 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=44387" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:44387" "--executor-id" "1" "--hostname" "172.16.16.101" "--cores" "1" "--app-id" "app-20210225115504-0003" "--worker-url" "spark://Worker@172.16.16.101:38706"
21/02/25 11:55:04 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=44387" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:44387" "--executor-id" "0" "--hostname" "172.16.16.102" "--cores" "1" "--app-id" "app-20210225115504-0003" "--worker-url" "spark://Worker@172.16.16.102:35340"
21/02/25 11:55:05 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:55:05 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:55:05 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:55:05 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:55:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:55:05 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=44387" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:44387" "--executor-id" "2" "--hostname" "172.16.16.103" "--cores" "1" "--app-id" "app-20210225115504-0003" "--worker-url" "spark://Worker@172.16.16.103:37264"
21/02/25 11:55:07 INFO Worker: Asked to kill executor app-20210225115504-0003/2
21/02/25 11:55:07 INFO Worker: Asked to kill executor app-20210225115504-0003/1
21/02/25 11:55:07 INFO ExecutorRunner: Runner thread for executor app-20210225115504-0003/2 interrupted
21/02/25 11:55:07 INFO ExecutorRunner: Killing process!
21/02/25 11:55:07 INFO ExecutorRunner: Runner thread for executor app-20210225115504-0003/1 interrupted
21/02/25 11:55:07 INFO ExecutorRunner: Killing process!
21/02/25 11:55:07 INFO Worker: Asked to kill executor app-20210225115504-0003/0
21/02/25 11:55:07 INFO ExecutorRunner: Runner thread for executor app-20210225115504-0003/0 interrupted
21/02/25 11:55:07 INFO ExecutorRunner: Killing process!
21/02/25 11:55:07 INFO Worker: Executor app-20210225115504-0003/2 finished with state KILLED exitStatus 143
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115504-0003, execId=2)
21/02/25 11:55:07 INFO Worker: Cleaning up local directories for application app-20210225115504-0003
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Application app-20210225115504-0003 removed, cleanupLocalDirs = true
21/02/25 11:55:07 INFO Worker: Executor app-20210225115504-0003/1 finished with state KILLED exitStatus 143
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115504-0003, execId=1)
21/02/25 11:55:07 INFO Worker: Cleaning up local directories for application app-20210225115504-0003
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Application app-20210225115504-0003 removed, cleanupLocalDirs = true
21/02/25 11:55:07 INFO Worker: Executor app-20210225115504-0003/0 finished with state KILLED exitStatus 143
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115504-0003, execId=0)
21/02/25 11:55:07 INFO Worker: Cleaning up local directories for application app-20210225115504-0003
21/02/25 11:55:07 INFO ExternalShuffleBlockResolver: Application app-20210225115504-0003 removed, cleanupLocalDirs = true
21/02/25 11:57:28 INFO Worker: Asked to launch executor app-20210225115728-0004/1 for pyspark-shell
21/02/25 11:57:28 INFO Worker: Asked to launch executor app-20210225115728-0004/0 for pyspark-shell
21/02/25 11:57:28 INFO Worker: Asked to launch executor app-20210225115728-0004/2 for pyspark-shell
21/02/25 11:57:28 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:57:28 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:57:28 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:57:28 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:57:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:57:28 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:57:28 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:57:28 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:57:28 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:57:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:57:28 INFO SecurityManager: Changing view acls to: laal
21/02/25 11:57:28 INFO SecurityManager: Changing modify acls to: laal
21/02/25 11:57:28 INFO SecurityManager: Changing view acls groups to: 
21/02/25 11:57:28 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 11:57:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 11:57:28 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=43036" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:43036" "--executor-id" "1" "--hostname" "172.16.16.101" "--cores" "1" "--app-id" "app-20210225115728-0004" "--worker-url" "spark://Worker@172.16.16.101:38706"
21/02/25 11:57:28 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=43036" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:43036" "--executor-id" "0" "--hostname" "172.16.16.102" "--cores" "1" "--app-id" "app-20210225115728-0004" "--worker-url" "spark://Worker@172.16.16.102:35340"
21/02/25 11:57:28 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java" "-cp" "/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/*" "-Xmx2000M" "-Dspark.driver.port=43036" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@desktop1.hpc.itu.dk:43036" "--executor-id" "2" "--hostname" "172.16.16.103" "--cores" "1" "--app-id" "app-20210225115728-0004" "--worker-url" "spark://Worker@172.16.16.103:37264"
21/02/26 09:50:35 INFO Worker: Asked to kill executor app-20210225115728-0004/2
21/02/26 09:50:35 INFO Worker: Asked to kill executor app-20210225115728-0004/1
21/02/26 09:50:35 INFO ExecutorRunner: Runner thread for executor app-20210225115728-0004/1 interrupted
21/02/26 09:50:35 INFO ExecutorRunner: Runner thread for executor app-20210225115728-0004/2 interrupted
21/02/26 09:50:35 INFO ExecutorRunner: Killing process!
21/02/26 09:50:35 INFO ExecutorRunner: Killing process!
21/02/26 09:50:35 INFO Worker: Asked to kill executor app-20210225115728-0004/0
21/02/26 09:50:35 INFO ExecutorRunner: Runner thread for executor app-20210225115728-0004/0 interrupted
21/02/26 09:50:35 INFO ExecutorRunner: Killing process!
21/02/26 09:50:36 INFO Worker: Executor app-20210225115728-0004/1 finished with state KILLED exitStatus 143
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115728-0004, execId=1)
21/02/26 09:50:36 INFO Worker: Cleaning up local directories for application app-20210225115728-0004
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Application app-20210225115728-0004 removed, cleanupLocalDirs = true
21/02/26 09:50:36 INFO Worker: Executor app-20210225115728-0004/0 finished with state KILLED exitStatus 143
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115728-0004, execId=0)
21/02/26 09:50:36 INFO Worker: Cleaning up local directories for application app-20210225115728-0004
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Application app-20210225115728-0004 removed, cleanupLocalDirs = true
21/02/26 09:50:36 INFO Worker: Executor app-20210225115728-0004/2 finished with state KILLED exitStatus 143
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20210225115728-0004, execId=2)
21/02/26 09:50:36 INFO ExternalShuffleBlockResolver: Application app-20210225115728-0004 removed, cleanupLocalDirs = true
21/02/26 09:50:36 INFO Worker: Cleaning up local directories for application app-20210225115728-0004
21/02/26 09:50:49 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /home/laal/MAG/TMP/spark-fcea78d1-2c0d-4456-83c0-e00d28f40e7d/executor-c87962a7-cff7-49a3-84c0-12093fd0f390. Falling back to Java IO way
java.io.IOException: Failed to delete: /home/laal/MAG/TMP/spark-fcea78d1-2c0d-4456-83c0-e00d28f40e7d/executor-c87962a7-cff7-49a3-84c0-12093fd0f390
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.deploy.worker.Worker.$anonfun$maybeCleanupApplication$5(Worker.scala:693)
	at org.apache.spark.deploy.worker.Worker.$anonfun$maybeCleanupApplication$5$adapted(Worker.scala:692)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.deploy.worker.Worker.$anonfun$maybeCleanupApplication$3(Worker.scala:692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/02/26 09:50:56 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /home/laal/MAG/TMP/spark-f2cc3918-2cb5-4cc2-951e-f62f420cfd80/executor-446b1300-86b5-4a8e-9ca6-e905c9692d0e. Falling back to Java IO way
java.io.IOException: Failed to delete: /home/laal/MAG/TMP/spark-f2cc3918-2cb5-4cc2-951e-f62f420cfd80/executor-446b1300-86b5-4a8e-9ca6-e905c9692d0e
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.deploy.worker.Worker.$anonfun$maybeCleanupApplication$5(Worker.scala:693)
	at org.apache.spark.deploy.worker.Worker.$anonfun$maybeCleanupApplication$5$adapted(Worker.scala:692)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.deploy.worker.Worker.$anonfun$maybeCleanupApplication$3(Worker.scala:692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 35731 ON desktop1 CANCELLED AT 2021-02-26T10:46:55 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 35731.1 ON desktop1 CANCELLED AT 2021-02-26T10:46:55 DUE TO TIME LIMIT ***
