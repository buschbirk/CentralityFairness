Running on desktop1:
INFO:sparkhpc.sparkjob:master command: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-master.sh
INFO:sparkhpc.sparkjob:[[92mstart_cluster] [0mmaster running at spark://desktop1:7077
INFO:sparkhpc.sparkjob:[[92mstart_cluster] [0mmaster UI available at http://desktop1.hpc.itu.dk:8080
I GOT THE SCHEDULER
slurm
master_command: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-master.sh
['desktop1', 'desktop2', 'desktop3']
INFO:sparkhpc.sparkjob:slaves command: srun /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-slave.sh spark://desktop1/127.0.0.1:7077 -c 1
INFO:sparkhpc.sparkjob:XXX SLAVES SLAVES: srun /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-slave.sh spark://desktop1/127.0.0.1:7077 -c 1
starting org.apache.spark.deploy.worker.Worker, logging to /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/logs/spark-laal-org.apache.spark.deploy.worker.Worker-1-desktop1.out
starting org.apache.spark.deploy.worker.Worker, logging to /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/logs/spark-laal-org.apache.spark.deploy.worker.Worker-1-desktop1.out
starting org.apache.spark.deploy.worker.Worker, logging to /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/logs/spark-laal-org.apache.spark.deploy.worker.Worker-1-desktop1.out
Spark Command: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java -cp /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://desktop1/127.0.0.1:7077 -c 1
========================================
Spark Command: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java -cp /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://desktop1/127.0.0.1:7077 -c 1
========================================
Spark Command: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/bin/java -cp /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/conf/:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://desktop1/127.0.0.1:7077 -c 1
========================================
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/02/25 10:28:26 INFO Worker: Started daemon with process name: 29562@desktop1
21/02/25 10:28:26 INFO SignalUtils: Registered signal handler for TERM
21/02/25 10:28:26 INFO SignalUtils: Registered signal handler for HUP
21/02/25 10:28:26 INFO SignalUtils: Registered signal handler for INT
21/02/25 10:28:26 WARN Utils: Your hostname, desktop1 resolves to a loopback address: 127.0.0.1; using 172.16.16.101 instead (on interface eno1)
21/02/25 10:28:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/02/25 10:28:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/25 10:28:27 INFO SecurityManager: Changing view acls to: laal
21/02/25 10:28:27 INFO SecurityManager: Changing modify acls to: laal
21/02/25 10:28:27 INFO SecurityManager: Changing view acls groups to: 
21/02/25 10:28:27 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 10:28:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 10:28:27 INFO Utils: Successfully started service 'sparkWorker' on port 46381.
21/02/25 10:28:27 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
org.apache.spark.SparkException: Invalid master URL: spark://desktop1/127.0.0.1:7077
	at org.apache.spark.util.Utils$.extractHostPortFromSparkUrl(Utils.scala:2399)
	at org.apache.spark.rpc.RpcAddress$.fromSparkURL(RpcAddress.scala:47)
	at org.apache.spark.deploy.worker.Worker$.$anonfun$startRpcEnvAndEndpoint$3(Worker.scala:860)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:860)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:829)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
21/02/25 10:28:27 INFO ShutdownHookManager: Shutdown hook called
srun: error: desktop1: task 0: Exited with exit code 1
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/02/25 10:28:28 INFO Worker: Started daemon with process name: 16212@desktop3
21/02/25 10:28:28 INFO SignalUtils: Registered signal handler for TERM
21/02/25 10:28:28 INFO SignalUtils: Registered signal handler for HUP
21/02/25 10:28:28 INFO SignalUtils: Registered signal handler for INT
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/02/25 10:28:29 WARN Utils: Your hostname, desktop3 resolves to a loopback address: 127.0.0.1; using 172.16.16.103 instead (on interface eno1)
21/02/25 10:28:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/02/25 10:28:29 INFO Worker: Started daemon with process name: 11878@desktop2
21/02/25 10:28:29 INFO SignalUtils: Registered signal handler for TERM
21/02/25 10:28:29 INFO SignalUtils: Registered signal handler for HUP
21/02/25 10:28:29 INFO SignalUtils: Registered signal handler for INT
21/02/25 10:28:29 WARN Utils: Your hostname, desktop2 resolves to a loopback address: 127.0.0.1; using 172.16.16.102 instead (on interface eno1)
21/02/25 10:28:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/02/25 10:28:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/25 10:28:29 INFO SecurityManager: Changing view acls to: laal
21/02/25 10:28:29 INFO SecurityManager: Changing modify acls to: laal
21/02/25 10:28:29 INFO SecurityManager: Changing view acls groups to: 
21/02/25 10:28:29 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 10:28:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/25 10:28:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 10:28:29 INFO SecurityManager: Changing view acls to: laal
21/02/25 10:28:29 INFO SecurityManager: Changing modify acls to: laal
21/02/25 10:28:29 INFO SecurityManager: Changing view acls groups to: 
21/02/25 10:28:29 INFO SecurityManager: Changing modify acls groups to: 
21/02/25 10:28:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(laal); groups with view permissions: Set(); users  with modify permissions: Set(laal); groups with modify permissions: Set()
21/02/25 10:28:29 INFO Utils: Successfully started service 'sparkWorker' on port 41556.
21/02/25 10:28:29 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
org.apache.spark.SparkException: Invalid master URL: spark://desktop1/127.0.0.1:7077
	at org.apache.spark.util.Utils$.extractHostPortFromSparkUrl(Utils.scala:2399)
	at org.apache.spark.rpc.RpcAddress$.fromSparkURL(RpcAddress.scala:47)
	at org.apache.spark.deploy.worker.Worker$.$anonfun$startRpcEnvAndEndpoint$3(Worker.scala:860)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:860)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:829)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
21/02/25 10:28:29 INFO ShutdownHookManager: Shutdown hook called
21/02/25 10:28:29 INFO Utils: Successfully started service 'sparkWorker' on port 45556.
21/02/25 10:28:29 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
org.apache.spark.SparkException: Invalid master URL: spark://desktop1/127.0.0.1:7077
	at org.apache.spark.util.Utils$.extractHostPortFromSparkUrl(Utils.scala:2399)
	at org.apache.spark.rpc.RpcAddress$.fromSparkURL(RpcAddress.scala:47)
	at org.apache.spark.deploy.worker.Worker$.$anonfun$startRpcEnvAndEndpoint$3(Worker.scala:860)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:860)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:829)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
21/02/25 10:28:29 INFO ShutdownHookManager: Shutdown hook called
srun: error: desktop3: task 2: Exited with exit code 1
srun: error: desktop2: task 1: Exited with exit code 1
INFO:sparkhpc.sparkjob:XXXX Finished slave commands
SLAVE COMMANDS: srun /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-slave.sh spark://desktop1/127.0.0.1:7077 -c 1
Executing
FINISHED SETTIN UP CLUSTER
INFO:sparkhpc.sparkjob:Submitted cluster -1
slurmstepd: error: *** JOB 35695 ON desktop1 CANCELLED AT 2021-02-25T10:30:25 ***
