Running on desktop1:
FINISHED SETTIN UP CLUSTER
21/02/25 11:45:20 WARN Utils: Your hostname, desktop1 resolves to a loopback address: 127.0.0.1; using 172.16.16.101 instead (on interface eno1)
21/02/25 11:45:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Ivy Default Cache set to: /home/laal/.ivy2/cache
The jars for the packages stored in: /home/laal/.ivy2/jars
:: loading settings :: url = jar:file:/home/laal/MAG/spark-3.0.2-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
graphframes#graphframes added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-349b212f-ea3c-4b07-8a93-4032f23cdad2;1.0
	confs: [default]
	found graphframes#graphframes;0.3.0-spark2.0-s_2.11 in spark-packages
	found com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central
	found com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central
	found org.scala-lang#scala-reflect;2.11.0 in central
	found org.slf4j#slf4j-api;1.7.7 in central
:: resolution report :: resolve 252ms :: artifacts dl 13ms
	:: modules in use:
	com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]
	com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]
	graphframes#graphframes;0.3.0-spark2.0-s_2.11 from spark-packages in [default]
	org.scala-lang#scala-reflect;2.11.0 from central in [default]
	org.slf4j#slf4j-api;1.7.7 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-349b212f-ea3c-4b07-8a93-4032f23cdad2
	confs: [default]
	0 artifacts copied, 5 already retrieved (0kB/21ms)
21/02/25 11:45:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/02/25 11:45:23 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
['NAME STATE JOBID', 'MAG_attribs RUNNING 35731', 'MAG_attribs RUNNING 35737', 'hebbian_weights_submit RUNNING 35734', 'hebbian_weights_submit RUNNING 35688', 'agg RUNNING 35571', 'agg RUNNING 35570', 'agg RUNNING 35569', 'agg RUNNING 35568', 'pen RUNNING 35567', 'pen RUNNING 35566', 'pen RUNNING 35565', 'pen RUNNING 35564', 'pen RUNNING 35563', 'agg RUNNING 35562', '']
Traceback (most recent call last):
  File "/home/laal/MAG/CentralityFairness/MAG_attributes_cluster.py", line 241, in <module>
    citations = citation_edges(mag)
  File "/home/laal/MAG/CentralityFairness/MAG_attributes_cluster.py", line 129, in citation_edges
    author_affiliations = mag.getDataframe('PaperAuthorAffiliations')
  File "/home/laal/MAG/CentralityFairness/MAG.py", line 51, in getDataframe
    df = self.spark.read.format('csv').options(header='false', delimiter='\t').schema(self.getSchema(streamName))\
AttributeError: 'SparkContext' object has no attribute 'read'
