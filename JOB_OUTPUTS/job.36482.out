Running on desktop9:
INFO:sparkhpc.sparkjob:master command: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-master.sh
no org.apache.spark.deploy.master.Master to stop
What is going on
I GOT THE SCHEDULER
slurm
master_command: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/sbin/start-master.sh
['desktop10', 'desktop11', 'desktop9']
Traceback (most recent call last):
  File "/home/laal/.conda/envs/torchenv/lib/python3.7/site-packages/sparkhpc/sparkjob.py", line 615, in start_cluster
    master_url, master_webui = re.findall('(spark://\S+:\d{4}|http://\S+:\d{4})', log)
ValueError: not enough values to unpack (expected 2, got 0)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/laal/MAG/CentralityFairness/clusterscript.py", line 16, in <module>
    spark_home='/home/laal/MAG/spark-3.0.2-bin-hadoop2.7')
  File "/home/laal/.conda/envs/torchenv/lib/python3.7/site-packages/sparkhpc/sparkjob.py", line 623, in start_cluster
    raise RuntimeError('Spark master appears to not be starting -- check the logs at: %s'%master_log)
RuntimeError: Spark master appears to not be starting -- check the logs at: /home/laal/MAG/spark-3.0.2-bin-hadoop2.7/logs/spark_master.out
