#!/bin/env
#SBATCH -J sparkcluster
#SBATCH -t 1410 # runtime to request !!! in minutes !!!
#SBATCH -o sparkcluster-%J.log # output extra o means overwrite
#SBATCH -n 12 # requesting n tasks
#SBATCH -c 1
#SBATCH --mem-per-cpu=8000 
#SBATCH -N 12
#SBATCH --ntasks-per-core=1

# setup the spark paths

echo "Running on $(hostname):"

module load Anaconda3
. $(conda info --base)/etc/profile.d/conda.sh
conda activate torchenv


python /home/laal/MAG/CentralityFairness/clusterscript.py


import os
os.environ['SPARK_HOME']='/home/laal/MAG/spark-3.0.2-bin-hadoop2.7'
os.environ['SPARK_LOCAL_DIRS']='/home/laal/MAG/TMP'
os.environ['LOCAL_DIRS']=os.environ['SPARK_LOCAL_DIRS']
os.environ['SPARK_WORKER_DIR']=os.path.join(os.environ['SPARK_LOCAL_DIRS'], 'work')

from sparkhpc import sparkjob

sparkjob.start_cluster('28000M', 
                       cores_per_executor=1, 
                       spark_home='/home/laal/MAG/spark-3.0.2-bin-hadoop2.7')
